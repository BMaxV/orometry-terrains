{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.coords import *\n",
    "from analysis.peaksdata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# font format for figures\n",
    "# see: https://stackoverflow.com/questions/33955900/matplotlib-times-new-roman-appears-bold\n",
    "del mpl.font_manager.weight_dict['roman']\n",
    "mpl.font_manager._rebuild()\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"font.weight\"] = 'normal'\n",
    "plt.rcParams['pdf.fonttype'] = 42 #TrueType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full names, for visualization and figures\n",
    "terrainFullName = {\n",
    "    'alaska': 'Alaska',\n",
    "    'alps': 'Alps',\n",
    "    'altai': 'Altai',\n",
    "    'andes_aconcagua': 'Andes-Aconcagua',\n",
    "    'andes_bolivia': 'Andes-Bolivia',\n",
    "    'andes_chile': 'Andes-Chile',\n",
    "    'andes_colombia': 'Andes-Colombia',\n",
    "    'andes_ecuador': 'Andes-Ecuador',\n",
    "    'andes_peru': 'Andes-Peru',\n",
    "    'apennines': 'Apennines',\n",
    "    'appalachians': 'Appalachians',\n",
    "    'turkey': 'Armenian-Highlands',\n",
    "    'atlas': 'Atlas',\n",
    "    'australia': 'Australia-GDR',\n",
    "    'balkans': 'Balkan-Peninsula',\n",
    "    'carpathian': 'Carpathians',\n",
    "    'cascades': 'Cascades',\n",
    "    'caucasus': 'Caucasus',\n",
    "    'southafrica': 'Drakensberg',\n",
    "    'ethiopia': 'Ethiopian-Highlands',\n",
    "    'gobi': 'Gobi',\n",
    "    'roraima': 'Guiana-Highlands',\n",
    "    'highlands': 'Scottish-Highlands',\n",
    "    'himalaya': 'Himalayas',\n",
    "    'hindukush': 'Hindu-Kush',\n",
    "    'iceland': 'Iceland',\n",
    "    'japan': 'Japan',\n",
    "    'kamchatka': 'Kamchatka',\n",
    "    'karakoram': 'Karakoram',\n",
    "    'kilimanjaro': 'Kilimanjaro',\n",
    "    'laos': 'Laos',\n",
    "    'yangshuo': 'Li-River',\n",
    "    'nevada': 'Nevada',\n",
    "    'newzealand': 'New-Zealand',\n",
    "    'papua': 'Papua-New-Guinea',\n",
    "    'pamir': 'Pamir',\n",
    "    'patagonia': 'Patagonia',\n",
    "    'pyrenees': 'Pyrenees',\n",
    "    'rockies': 'Rockies-Canadian',\n",
    "    'colorado': 'Rockies-Colorado',\n",
    "    'sahara': 'Sahara',\n",
    "    'norway': 'Scandes-Norway',\n",
    "    'siberia': 'Siberia',\n",
    "    'mexico': 'Sierra-Madre',\n",
    "    'sierra': 'Sierra-Nevada',\n",
    "    'taurus': 'Taurus',\n",
    "    'tibet': 'Tibetan-Plateau',\n",
    "    'tienshan': 'Tien-Shan',\n",
    "    'urals': 'Urals',\n",
    "    'zagros': 'Zagros'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the 15 terrains used for the small confusion matrix in the article\n",
    "chosenTerrains15 = [\n",
    "    'alps',\n",
    "    'himalaya',\n",
    "    'karakoram',\n",
    "    'norway',\n",
    "    'sahara',\n",
    "    'appalachians',\n",
    "    'andes_chile',\n",
    "    'newzealand',\n",
    "    'rockies',\n",
    "    'andes_peru',\n",
    "    'alaska',\n",
    "    'patagonia',\n",
    "    'colorado',\n",
    "    'caucasus',\n",
    "    'gobi'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dataset path\n",
    "dsWorld = 'data/regions_30km.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "regionsDataset = pd.read_csv(dsWorld, sep=',')\n",
    "print('Dataset size', regionsDataset.shape)\n",
    "\n",
    "# terrain labels in set\n",
    "terrainLabels = np.unique(regionsDataset['terrain'])\n",
    "print(terrainLabels.size, terrainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which terrains do we want to use?\n",
    "#chosenTerrains = sorted(chosenTerrains15, key=lambda x: terrainFullName[x])\n",
    "chosenTerrains = sorted(terrainLabels, key=lambda x: terrainFullName[x])\n",
    "\n",
    "# check our dataset\n",
    "print('Chosen terrains:', np.unique(chosenTerrains).size)\n",
    "print(chosenTerrains)\n",
    "\n",
    "print()\n",
    "print('Under-sampled terrains')\n",
    "for t in chosenTerrains:\n",
    "    if np.sum(regionsDataset['terrain'] == t) < 100:\n",
    "        print('    ', np.sum(regionsDataset['terrain'] == t), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append number of samples to terrain name?\n",
    "appendSamplesToName = True\n",
    "if appendSamplesToName:\n",
    "    for t in terrainLabels:\n",
    "        # under-represented terrains, append samples to name\n",
    "        if appendSamplesToName and np.sum(regionsDataset['terrain'] == t) < 100:\n",
    "            terrainFullName[t] = terrainFullName[t] + ' (' + str(np.sum(regionsDataset['terrain'] == t)) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to get a train/val split\n",
    "def getTrainVal(df, chosenTerrains, maxTrain = 80, maxValid = 20):\n",
    "    terrainRows = {}\n",
    "    for t in chosenTerrains:\n",
    "        terrainRows[t] = df.index[df['terrain'] == t]\n",
    "\n",
    "    terrainShuffledRows = {}\n",
    "    for t in chosenTerrains:\n",
    "        terrainShuffledRows[t] = np.random.permutation(terrainRows[t])\n",
    "\n",
    "    trainSet = []\n",
    "    validSet = []\n",
    "    percTrain = maxTrain/(maxTrain + maxValid)\n",
    "    for t in chosenTerrains:\n",
    "        nSamples = terrainShuffledRows[t].size\n",
    "        nTrain   = np.minimum(maxTrain, np.round(percTrain*nSamples).astype(np.int))\n",
    "        nValid   = np.minimum(maxValid, nSamples - nTrain)\n",
    "        for i in range(nTrain):\n",
    "            trainSet.append(terrainShuffledRows[t][i])\n",
    "        for i in range(nValid):\n",
    "            validSet.append(terrainShuffledRows[t][nTrain + i])\n",
    "            \n",
    "\n",
    "    return trainSet, validSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to test only a set of the metrics\n",
    "def getFeatureColumns(df, keepFeatures):\n",
    "    # each feature contains many columns of the dataset (the histogram bins), so keep all of them\n",
    "    keepCols = []\n",
    "    for c in df.columns:\n",
    "        if c.split('_')[0] in keepFeatures:\n",
    "            keepCols.append(c)\n",
    "        \n",
    "    return keepCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix (Accuracy %)',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = np.round(100*cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]).astype(int)\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "        \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=18)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    #ax.xaxis.set_ticks_position('both')\n",
    "    #ax.xaxis.set_tick_params(labeltop=True)\n",
    "\n",
    "    fmt = 'd' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 verticalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=18)\n",
    "    plt.xlabel('Predicted label', fontsize=18)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Regions classifier accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of experiments\n",
    "numRuns = 100\n",
    "\n",
    "# we can experiment with only a subset of metrics (features)\n",
    "# for example, note that we excluded 'elevation'\n",
    "keepCols = getFeatureColumns(regionsDataset,\n",
    "                             ['elevRel', 'prominence', 'promRel', \n",
    "                              'dominance', 'domGroup', 'relevance',\n",
    "                              'isolation', 'isolDir', 'saddleDist', 'saddleDir'])\n",
    "print(keepCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's execute the test and store the accumulated confusion matrix from all the runs\n",
    "\n",
    "# confusion matrix\n",
    "C = np.zeros((len(chosenTerrains), len(chosenTerrains)))\n",
    "\n",
    "# accuracies\n",
    "scores = []\n",
    "for i in range(numRuns):    \n",
    "    \n",
    "    print('Run %3d/%3d' % (i+1,numRuns), end='\\r' if i+1 < numRuns else '\\n')\n",
    "    \n",
    "    trainSet, validSet = getTrainVal(regionsDataset, chosenTerrains, maxTrain=80, maxValid=20)\n",
    "    Xt = regionsDataset[keepCols].values[trainSet,:]\n",
    "    yt = regionsDataset['terrain'].values[trainSet]\n",
    "    Xv = regionsDataset[keepCols].values[validSet,:]\n",
    "    yv = regionsDataset['terrain'].values[validSet]\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(Xt, yt)\n",
    "    scores.append(model.score(Xv, yv))\n",
    "    \n",
    "    yp = model.predict(Xv)\n",
    "    C += confusion_matrix(yv, yp, labels=chosenTerrains)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean accuracy:', '%.2f'%(100*np.mean(scores)), '%.2f'%(100*np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classAcc = np.diag((C.astype('float') / C.sum(axis=1)[:, np.newaxis]))\n",
    "classByAcc = np.argsort(classAcc)[::-1]\n",
    "\n",
    "for c in classByAcc:\n",
    "    print(chosenTerrains[c], '%.1f'%(classAcc[c]*100))\n",
    "    \n",
    "print()\n",
    "print('Median: ', '%.1f'%(100*np.median(classAcc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "renamedTerrains = [terrainFullName[t] for t in chosenTerrains]\n",
    "\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "plot_confusion_matrix(C, classes=renamedTerrains, title='Confusion matrix (Accuracy %)', normalize=True)\n",
    "fig.savefig('confusionMatrix.pdf', dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Classify a terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample disk radius (should be the same as the one used to construct the dataset!)\n",
    "diskRadius = 30 # km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input peaks file\n",
    "terrainPeaksFile = 'results/synth_alps.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the peaks csv\n",
    "peaks = pd.read_csv(terrainPeaksFile)\n",
    "peaks = addExtraColumns(peaks)\n",
    "\n",
    "# normalize distance columns\n",
    "peaks['isolation']  /= diskRadius\n",
    "peaks['saddleDist'] /= diskRadius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute BBox of terrain\n",
    "minLat = peaks['latitude'].min()\n",
    "maxLat = peaks['latitude'].max()\n",
    "minLon = peaks['longitude'].min()\n",
    "maxLon = peaks['longitude'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range for sampling random positions. If 0, we will always sample at center location\n",
    "ctrLon = 0.5*(minLon + maxLon)\n",
    "ctrLat = 0.5*(minLat + maxLat)\n",
    "hrangeLat = np.maximum(0, 0.5*(maxLat - minLat) - km2deg(diskRadius))\n",
    "hrangeLon = np.maximum(0, 0.5*(maxLon - minLon) - km2deg(diskRadius, ctrLat))\n",
    "\n",
    "print('Center', ctrLat, ctrLon)\n",
    "print('Range ', hrangeLat, hrangeLon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different locations we want to try\n",
    "numSampleLocations = 20\n",
    "\n",
    "# obtain positions\n",
    "rndOffset = np.random.uniform(-1, 1, size=(numSampleLocations, 2))\n",
    "sampleLocations = rndOffset*np.array([hrangeLat, hrangeLon]) + np.array([ctrLat, ctrLon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the distributions at each sampled location\n",
    "sampledDistribs = {}\n",
    "for c in regionsDataset.columns:\n",
    "    if c == 'terrain':\n",
    "        continue\n",
    "    sampledDistribs[c] = []\n",
    "\n",
    "for i,loc in enumerate(sampleLocations):\n",
    "\n",
    "    # peaks inside disk\n",
    "    locPeaks = filterPeaksHaversineDist(peaks, loc, diskRadius)\n",
    "    npeaks = locPeaks.shape[0]\n",
    "    \n",
    "    # metrics distributions\n",
    "    dists = computeDistributions(locPeaks, diskRadius=1.0, detailed=False)\n",
    "    \n",
    "    # append to arrays\n",
    "    for feat in ['elevation', 'prominence', 'isolDir', 'saddleDir']:\n",
    "        for hbin,hval in zip(dists[feat]['bins'][:-1], dists[feat]['hist']):\n",
    "            sampledDistribs['%s_%d' % (feat, int(hbin))].append(hval/npeaks)                             \n",
    "    for feat in ['domGroup']:\n",
    "        for hbin,hval in zip(dists[feat]['bins'][:-1], dists[feat]['hist']):\n",
    "            sampledDistribs['%s_%.2f' % (feat, 100*hbin)].append(hval/npeaks)                  \n",
    "    for feat in ['elevRel', 'promRel', 'dominance', 'relevance', 'isolation', 'saddleDist']:\n",
    "        for hbin,hval in zip(dists[feat]['bins'][:-1], dists[feat]['hist']):\n",
    "            sampledDistribs['%s_%.2f' % (feat, hbin)].append(hval/npeaks)\n",
    "            \n",
    "    print('Sampled location %3d/%3d' % (i+1,numSampleLocations), end='\\r' if i+1 < numSampleLocations else '\\n')\n",
    "\n",
    "# dataframe\n",
    "samplesDataset = pd.DataFrame.from_dict(sampledDistribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many classifications per sample?\n",
    "numTests = 100\n",
    "\n",
    "# all features used to evaluate terrain classification previously\n",
    "allClassifierFeatures =  ['elevRel', 'prominence', 'promRel', \n",
    "                          'dominance', 'domGroup', 'relevance',\n",
    "                          'isolation', 'isolDir', 'saddleDist', 'saddleDir']\n",
    "# only the features that were taken into account in our synthesis algorithm\n",
    "synthesisFeatures = ['elevRel', 'prominence', 'dominance', 'isolation', 'isolDir']\n",
    "\n",
    "keepCols = getFeatureColumns(regionsDataset, allClassifierFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the tests\n",
    "predictedRegions = {}\n",
    "\n",
    "for i in range(numTests):\n",
    "    \n",
    "    print('Test %3d/%3d' % (i+1,numTests), end='\\r' if i+1 < numTests else '\\n')\n",
    "\n",
    "    # train model\n",
    "    trainSet, _ = getTrainVal(regionsDataset, chosenTerrains, maxTrain=100, maxValid=0)\n",
    "    Xt = regionsDataset[keepCols].values[trainSet,:]\n",
    "    yt = regionsDataset['terrain'].values[trainSet]\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(Xt, yt)\n",
    "\n",
    "    # predict\n",
    "    preds = model.predict(samplesDataset[keepCols])\n",
    "    \n",
    "    # sum predictions\n",
    "    for p in preds:\n",
    "        predictedRegions[p] = predictedRegions.get(p, 0) + 1\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw predictions\n",
    "print(predictedRegions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of each predicted class\n",
    "sortedPredictions = sorted(predictedRegions.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for k,v in sortedPredictions:\n",
    "    print('%5.2f - %s' % (100*v/(numTests*numSampleLocations), terrainFullName[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
